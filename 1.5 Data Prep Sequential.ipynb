{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86c253c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/nielsen/churn/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1d291ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_869563/2519247929.py:6: DtypeWarning: Columns (20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_part3 = pd.read_csv(\"received_csvs/eierverifiseringer__historikk_part3.csv\")\n",
      "/tmp/ipykernel_869563/2519247929.py:8: DtypeWarning: Columns (20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_part5 = pd.read_csv(\"received_csvs/eierverifiseringer__historikk_part5.csv\")\n"
     ]
    }
   ],
   "source": [
    "abonnementsprodukter__historikk_df = pd.read_csv('received_csvs/abonnementsprodukter__historikk.csv')\n",
    "\n",
    "\n",
    "df_part1 = pd.read_csv(\"received_csvs/eierverifiseringer__historikk_part1.csv\")\n",
    "df_part2 = pd.read_csv(\"received_csvs/eierverifiseringer__historikk_part2.csv\")\n",
    "df_part3 = pd.read_csv(\"received_csvs/eierverifiseringer__historikk_part3.csv\")\n",
    "df_part4 = pd.read_csv(\"received_csvs/eierverifiseringer__historikk_part4.csv\")\n",
    "df_part5 = pd.read_csv(\"received_csvs/eierverifiseringer__historikk_part5.csv\")\n",
    "df_part6 = pd.read_csv(\"received_csvs/eierverifiseringer__historikk_part6.csv\")\n",
    "eierverifiseringer__historikk_df = pd.concat([df_part1, df_part2, df_part3, df_part4, df_part5, df_part6], ignore_index=True)\n",
    "\n",
    "produktuttak_df = pd.read_csv('received_csvs/produktuttak.csv')\n",
    "stg_dynamics__contacts_df = pd.read_csv('received_csvs/stg_dynamics__contacts.csv')\n",
    "nps_svar_df = pd.read_csv('received_csvs/nps_svar.csv').drop(columns=\"contact_rk.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82308a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "abonnementsprodukter__historikk_df.rename(columns={'subscriptionitem__contact_rk':'contact_rk'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb3f8c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10444260"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eierverifiseringer__historikk_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b471a2",
   "metadata": {},
   "source": [
    "### Only store rows with valid membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7700dcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eierverifiseringer__historikk_df = eierverifiseringer__historikk_df[eierverifiseringer__historikk_df['contact_rk'].isin(abonnementsprodukter__historikk_df['contact_rk'])]\n",
    "produktuttak_df = produktuttak_df[produktuttak_df['contact_rk'].isin(abonnementsprodukter__historikk_df['contact_rk'])]\n",
    "stg_dynamics__contacts_df = stg_dynamics__contacts_df[stg_dynamics__contacts_df['contact_rk'].isin(abonnementsprodukter__historikk_df['contact_rk'])]\n",
    "nps_svar_df = nps_svar_df[nps_svar_df['contact_rk'].isin(abonnementsprodukter__historikk_df['contact_rk'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca363617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7684725"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eierverifiseringer__historikk_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e874ad74",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dffbd3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs:\n",
      "innmeldingsdato    102811\n",
      "startdato               0\n",
      "sluttdato          524027\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count NaNs in specific columns\n",
    "nan_counts = (\n",
    "    abonnementsprodukter__historikk_df[[\"innmeldingsdato\", \"startdato\", \"sluttdato\"]]\n",
    "    .isna()\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "print(\"Number of NaNs:\")\n",
    "print(nan_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85cdd3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non-unique contact_rk: 0\n",
      "Series([], Name: count, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Find duplicated contact_rk values\n",
    "duplicates = (\n",
    "    stg_dynamics__contacts_df[\"contact_rk\"]\n",
    "    .value_counts()\n",
    "    .loc[lambda x: x > 1]\n",
    ")\n",
    "\n",
    "# Display how many contact_rk are duplicated\n",
    "print(f\"Number of non-unique contact_rk: {duplicates.shape[0]}\")\n",
    "\n",
    "# If you want to see the list:\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c1daac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age    58195\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# abonnementsprodukter__historikk_df = abonnementsprodukter__historikk_df.dropna(subset=['innmeldingsdato', 'sluttdato'])\n",
    "abonnementsprodukter__historikk_df = abonnementsprodukter__historikk_df.dropna(subset=['sluttdato'])\n",
    "nan_count = stg_dynamics__contacts_df[['age']].isna().sum()\n",
    "stg_dynamics__contacts_df = stg_dynamics__contacts_df.dropna(subset=['age'])\n",
    "\n",
    "print(\"Rows with age NaN dropped:\\n\", nan_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6b0ea9",
   "metadata": {},
   "source": [
    "# Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3c244d",
   "metadata": {},
   "source": [
    "### Only create a year row if, for that year, the sluttdato is at least 3 weeks (21 days) past the anniversary of the startdato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "64efb560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 766002/766002 [00:38<00:00, 19810.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number rows coerced:\n",
      " startdato    2\n",
      "sluttdato    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "TODAY = pd.Timestamp(\"2025-01-01\")\n",
    "\n",
    "# 1. Parse and clean dates\n",
    "sub = abonnementsprodukter__historikk_df.copy()\n",
    "\n",
    "sub[\"startdato\"] = pd.to_datetime(sub[\"startdato\"], errors=\"coerce\")\n",
    "sub[\"sluttdato\"] = pd.to_datetime(sub[\"sluttdato\"], errors=\"coerce\").fillna(TODAY)\n",
    "\n",
    "number_dropped = sub[[\"startdato\", \"sluttdato\"]].isna().sum()\n",
    "sub = sub.dropna(subset=[\"startdato\", \"sluttdato\"])\n",
    "\n",
    "# 2. Filter unreasonable dates\n",
    "sub = sub[\n",
    "    (sub[\"startdato\"].dt.year >= 1970)\n",
    "    & (sub[\"sluttdato\"].dt.year >= 1970)\n",
    "    & (sub[\"sluttdato\"] <= TODAY)\n",
    "]\n",
    "\n",
    "# 3. GAP DETECTION: Split contacts based on gap > 1y + 3w\n",
    "GAP_THRESHOLD = pd.Timedelta(days=365 + 21)\n",
    "\n",
    "sub = sub.sort_values([\"contact_rk\", \"startdato\"])\n",
    "sub[\"prev_sluttdato\"] = sub.groupby(\"contact_rk\")[\"sluttdato\"].shift()\n",
    "sub[\"gap\"] = sub[\"startdato\"] - sub[\"prev_sluttdato\"]\n",
    "\n",
    "sub[\"new_membership\"] = (\n",
    "    (sub[\"gap\"] > GAP_THRESHOLD) | (sub[\"prev_sluttdato\"].isna())\n",
    ").astype(int)\n",
    "\n",
    "sub[\"membership_number\"] = sub.groupby(\"contact_rk\")[\"new_membership\"].cumsum()\n",
    "\n",
    "sub[\"contact_rk_gap\"] = sub[\"contact_rk\"] + \"-\" + sub[\"membership_number\"].astype(str)\n",
    "\n",
    "sub = sub.drop(columns=[\"prev_sluttdato\", \"gap\", \"new_membership\", \"membership_number\"])\n",
    "\n",
    "\n",
    "# 4. Generate year_list (only if sluttdato > anniversary + 3 weeks)\n",
    "def generate_year_list(row):\n",
    "    years = []\n",
    "    for year in range(row.startdato.year, row.sluttdato.year + 1):\n",
    "        month = row.startdato.month\n",
    "        day = row.startdato.day\n",
    "        if month == 2 and day == 29:\n",
    "            if not (year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)):\n",
    "                day = 28  # Handle non-leap years\n",
    "        anniversary = pd.Timestamp(year=year, month=month, day=day)\n",
    "        if row.sluttdato >= anniversary + pd.Timedelta(days=21):\n",
    "            years.append(year)\n",
    "    return years\n",
    "\n",
    "\n",
    "sub[\"year_list\"] = sub.progress_apply(generate_year_list, axis=1)\n",
    "\n",
    "# 5. Explode into membership years\n",
    "membership_years = (\n",
    "    sub.explode(\"year_list\", ignore_index=True)\n",
    "    .rename(columns={\"year_list\": \"year\"})\n",
    "    .dropna(subset=[\"year\"])\n",
    "    .groupby(\n",
    "        [\n",
    "            \"contact_rk_gap\",  # ← keep your base ID\n",
    "            \"contact_rk\",  # ← the suffixed membership ID\n",
    "            \"year\",\n",
    "        ],\n",
    "        as_index=False,\n",
    "    )\n",
    "    .agg(is_household=(\"er_husstandsmedlem\", \"max\"))\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Number rows coerced:\\n\", number_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf31c613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create price table\n",
    "price_data = [\n",
    "    (\"2025-01-01\", 1520),\n",
    "    (\"2024-01-15\", 1440),\n",
    "    (\"2023-01-15\", 1380),\n",
    "    (\"2021-01-15\", 1320),\n",
    "    (\"2020-01-15\", 1270),\n",
    "    (\"2019-01-15\", 1220),\n",
    "    (\"2018-01-15\", 1190),\n",
    "    (\"2017-01-15\", 1140),\n",
    "    (\"2016-01-15\", 1120),\n",
    "    (\"2014-01-15\", 1095),\n",
    "    (\"2013-01-15\", 1060),\n",
    "    (\"2012-01-15\", 1035),\n",
    "    (\"2011-01-15\", 980),\n",
    "    (\"2009-01-15\", 930),\n",
    "]\n",
    "\n",
    "price_df = pd.DataFrame(price_data, columns=[\"effective_date\", \"price\"])\n",
    "price_df[\"effective_date\"] = pd.to_datetime(price_df[\"effective_date\"])\n",
    "\n",
    "# Remove duplicates, keep the latest price\n",
    "price_df = price_df.sort_values(\"effective_date\").drop_duplicates(\n",
    "    subset=\"effective_date\", keep=\"last\"\n",
    ")\n",
    "\n",
    "# Create a year column for easier join\n",
    "price_df[\"year\"] = price_df[\"effective_date\"].dt.year\n",
    "\n",
    "# Now match the correct price to each year\n",
    "# For each membership_years['year'], find the most recent price year <= that year\n",
    "\n",
    "membership_years[\"year\"] = membership_years[\"year\"].astype(int)\n",
    "\n",
    "# Make sure 'year' is the same dtype in both DataFrames\n",
    "membership_years[\"year\"] = membership_years[\"year\"].astype(\"int64\")\n",
    "price_df[\"year\"] = price_df[\"year\"].astype(\"int64\")\n",
    "\n",
    "# Sort both\n",
    "membership_years = membership_years.sort_values(\"year\")\n",
    "price_df = price_df.sort_values(\"year\")\n",
    "\n",
    "# Merge\n",
    "membership_years = pd.merge_asof(\n",
    "    membership_years, price_df[[\"year\", \"price\"]], on=\"year\", direction=\"backward\"\n",
    ")\n",
    "\n",
    "# Replace NaN prices with -1\n",
    "membership_years[\"price\"] = membership_years[\"price\"].fillna(-1)\n",
    "\n",
    "# Re-sort by contact_rk and year\n",
    "membership_years = membership_years.sort_values([\"contact_rk_gap\", \"year\"]).reset_index(\n",
    "    drop=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e19f335",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_869563/2643968144.py:37: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  veh[col] = pd.to_datetime(veh[col], errors=\"coerce\")\n",
      "/tmp/ipykernel_869563/2643968144.py:37: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  veh[col] = pd.to_datetime(veh[col], errors=\"coerce\")\n",
      "100%|██████████| 7682018/7682018 [00:59<00:00, 129872.61it/s]\n",
      "100%|██████████| 7682018/7682018 [00:59<00:00, 129551.86it/s]\n"
     ]
    }
   ],
   "source": [
    "veh = eierverifiseringer__historikk_df[\n",
    "    [\n",
    "        \"contact_rk\",\n",
    "        \"vehicle_rk\",\n",
    "        \"registert_første_gang_dato\",\n",
    "        \"forrige_eu_kontroll_dato\",\n",
    "        \"neste_eu_kontroll_dato\",\n",
    "        \"vraket_dato\",\n",
    "        \"årsmodell\",\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "# robust parsing for every date-column in that table\n",
    "for col in [\n",
    "    \"vraket_dato\",\n",
    "    \"forrige_eu_kontroll_dato\",\n",
    "    \"neste_eu_kontroll_dato\",\n",
    "    \"registert_første_gang_dato\",\n",
    "]:\n",
    "    veh[col] = pd.to_datetime(veh[col], format=\"mixed\", errors=\"coerce\")\n",
    "\n",
    "veh[\"reg_start\"] = veh[\"registert_første_gang_dato\"]  # rename for brevity\n",
    "\n",
    "# robust parsing for every date-column in that table\n",
    "for col in [\n",
    "    \"vraket_dato\",\n",
    "    \"forrige_eu_kontroll_dato\",\n",
    "    \"neste_eu_kontroll_dato\",\n",
    "    \"registert_første_gang_dato\",\n",
    "]:\n",
    "\n",
    "    # Apply string replacement only if the column is not yet datetime\n",
    "    veh[col] = veh[col].astype(str).str.replace(\"T\", \" \", regex=False)\n",
    "    veh[col] = pd.to_datetime(veh[col], errors=\"coerce\")\n",
    "\n",
    "veh[\"vraket_dato\"] = pd.to_datetime(veh[\"vraket_dato\"])\n",
    "veh[\"reg_start\"] = pd.to_datetime(veh[\"registert_første_gang_dato\"])\n",
    "veh[\"start_year\"] = veh[\"reg_start\"].dt.year.fillna(veh[\"årsmodell\"])\n",
    "veh[\"end_year\"] = veh[\"vraket_dato\"].dt.year.fillna(TODAY.year)\n",
    "\n",
    "# Remove rows where start or end year is missing\n",
    "veh = veh.dropna(subset=[\"start_year\", \"end_year\"])\n",
    "\n",
    "# Ensure start_year and end_year are integers\n",
    "veh[\"start_year\"] = veh[\"start_year\"].astype(int)\n",
    "veh[\"end_year\"] = veh[\"end_year\"].astype(int)\n",
    "\n",
    "# Build the year list\n",
    "veh[\"year_list\"] = veh.progress_apply(\n",
    "    lambda r: list(range(r.start_year, r.end_year + 1)), axis=1\n",
    ")\n",
    "\n",
    "\n",
    "veh[\"year_list\"] = veh.progress_apply(\n",
    "    lambda r: list(range(int(r.start_year), int(r.end_year) + 1)), axis=1\n",
    ")\n",
    "veh_exp = veh.explode(\"year_list\", ignore_index=True).rename(\n",
    "    columns={\"year_list\": \"year\"}\n",
    ")\n",
    "veh_exp[\"vehicle_age\"] = veh_exp[\"year\"] - veh_exp[\"årsmodell\"]\n",
    "\n",
    "vehicle_yearly = veh_exp.groupby([\"contact_rk\", \"year\"], as_index=False).agg(\n",
    "    vehicle_count=(\"vehicle_rk\", \"nunique\"), vehicle_age_mean=(\"vehicle_age\", \"mean\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00aa0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pu = produktuttak_df.copy()\n",
    "pu[\"opprettet_dato\"] = pd.to_datetime(pu[\"opprettet_dato\"])\n",
    "pu[\"year\"] = pu[\"opprettet_dato\"].dt.year\n",
    "belop_yearly = pu.groupby([\"contact_rk\", \"year\"], as_index=False)[\"beløp\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d982ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fb = nps_svar_df.copy()\n",
    "fb[\"opprettet_dato\"] = pd.to_datetime(fb[\"opprettet_dato\"])\n",
    "fb[\"year\"] = fb[\"opprettet_dato\"].dt.year\n",
    "feedback_yearly = fb.groupby([\"contact_rk\", \"year\"], as_index=False).agg(\n",
    "    gave_feedback=(\"score\", \"size\"), mean_feedback=(\"score\", \"mean\")\n",
    ")\n",
    "feedback_yearly[\"gave_feedback\"] = feedback_yearly[\"gave_feedback\"] > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecbc38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "contacts = stg_dynamics__contacts_df[\n",
    "    [\"contact_rk\", \"birth_date\", \"gender\", \"sentralitetsindex\"]\n",
    "].copy()\n",
    "contacts[\"birth_date\"] = pd.to_datetime(contacts[\"birth_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450a514c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_869563/18373046.py:14: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  ].fillna(0)\n",
      "/tmp/ipykernel_869563/18373046.py:15: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"gave_feedback\"] = df[\"gave_feedback\"].fillna(False)\n"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "    membership_years.merge(vehicle_yearly, on=[\"contact_rk\", \"year\"], how=\"left\")\n",
    "    .merge(belop_yearly, on=[\"contact_rk\", \"year\"], how=\"left\")\n",
    "    .merge(feedback_yearly, on=[\"contact_rk\", \"year\"], how=\"left\")\n",
    "    .merge(contacts, on=\"contact_rk\", how=\"left\")\n",
    ")\n",
    "\n",
    "# fill NA for numeric columns that should default to 0\n",
    "df[[\"vehicle_count\", \"vehicle_age_mean\", \"beløp\", \"mean_feedback\"]] = df[\n",
    "    [\"vehicle_count\", \"vehicle_age_mean\", \"beløp\", \"mean_feedback\"]\n",
    "].fillna(0)\n",
    "df[\"gave_feedback\"] = df[\"gave_feedback\"].fillna(False)\n",
    "\n",
    "# on-the-fly age in that calendar year\n",
    "df[\"age\"] = df[\"year\"] - df[\"birth_date\"].dt.year\n",
    "\n",
    "\n",
    "# Assign sequential year numbers per member\n",
    "df[\"membership_year\"] = (\n",
    "    df.groupby(\"contact_rk_gap\")[\"year\"].rank(method=\"dense\").astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3fc2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_active_year = (\n",
    "    membership_years.groupby(\"contact_rk_gap\")[\"year\"].max().rename(\"last_year\")\n",
    ")\n",
    "df = df.merge(last_active_year, on=\"contact_rk_gap\")\n",
    "df[\"churn\"] = ((df[\"year\"] == df[\"last_year\"]) & (df[\"last_year\"] < TODAY.year)).astype(\n",
    "    int\n",
    ")\n",
    "df = df.drop(columns=[\"last_year\", \"contact_rk_clean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d445d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_cols = [\n",
    "    \"contact_rk_gap\",\n",
    "    \"membership_year\",\n",
    "    \"year\",\n",
    "    \"churn\",\n",
    "    \"price\",\n",
    "    \"is_household\",\n",
    "    \"vehicle_count\",\n",
    "    \"vehicle_age_mean\",\n",
    "    \"beløp\",\n",
    "    \"gave_feedback\",\n",
    "    \"mean_feedback\",\n",
    "    \"age\",\n",
    "    \"gender\",\n",
    "    \"sentralitetsindex\",\n",
    "]\n",
    "df = df[ordered_cols].sort_values([\"contact_rk_gap\", \"year\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "30fa2d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before removing NaNs: 2461168\n",
      "Rows after removing NaNs: 2397032\n"
     ]
    }
   ],
   "source": [
    "# Print the number of rows before removing NaNs\n",
    "print(\"Rows before removing NaNs:\", len(df))\n",
    "\n",
    "# Remove all rows with any NaN values\n",
    "df = df.dropna()\n",
    "\n",
    "# Print the number of rows after removing NaNs\n",
    "print(\"Rows after removing NaNs:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3d479a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "uid = str(uuid.uuid4())\n",
    "\n",
    "df.to_parquet(f'df-{uid}.parquet.gzip', compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
